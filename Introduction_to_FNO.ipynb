{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64cb0bb-cbd7-4bda-84a7-712a530c3369",
   "metadata": {},
   "source": [
    "## Fourier neural operator\n",
    "- [Fourier Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/pdf/2010.08895)\n",
    "- [Operator Learing - Fourier Neural Operator](https://github.com/camlab-ethz/AI_Science_Engineering/blob/main/Tutorial%2005%20-%20Operator%20Learing%20-%20Fourier%20Neural%20Operator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69082641-9750-4f8a-93ac-b9db6fc7c7a9",
   "metadata": {},
   "source": [
    "## (1) Review of Parametric PDE Problems\n",
    "\n",
    "We consider a general parametric PDE in residual form:\n",
    "$$\n",
    "F(x,y,u,u_x,u_y,u_{xx},u_{yy}, \\cdots, a) = 0,\\quad (x,y)\\in\\Omega,\n",
    "$$\n",
    "where:\n",
    "- $u(x,y)$ is the solution,\n",
    "- $a(x,y)$ is a parameter (e.g., conductivity or source) that may vary across instances,\n",
    "- and appropriate boundary conditions are applied on $\\partial\\Omega$ (Dirichlet, Neumann, or mixed).\n",
    "\n",
    "The goal is to solve for $u(x,y;a)$ given any input parameter $a(x,y)$. This defines an operator learning problem, where we seek to approximate the solution operator:\n",
    "$$\n",
    "\\mathcal{G}: a(x,y)\\in\\mathcal{A} \\longrightarrow u(x,y)\\in\\mathcal{U}, \\tag{1.1}\n",
    "$$\n",
    "with $\\mathcal{A}$ and $\\mathcal{U}$ being function spaces for the inputs and outputs, respectively.\n",
    "\n",
    "## (2) Kernel-Based Neural Operator\n",
    "\n",
    "### (2.1) Standard Neural Network Layer\n",
    "\n",
    "In classical neural networks, we apply a sequence of linear transformations and nonlinear activations to vectors. A generic solution operator can be written as:\n",
    "$$\n",
    "u = (K_L \\circ \\sigma_{L-1} \\circ \\cdots \\circ \\sigma_1 \\circ K_0)(v),\n",
    "$$\n",
    "where each $K_l$ is a linear or convolutional layer, and $\\sigma_l$ is a nonlinear activation (e.g., ReLU). The layer-wise mapping is as follows:\n",
    "$$\n",
    "v_{l+1} = \\sigma_l(K_l v_l + W_l),\n",
    "$$\n",
    "with $\\sigma_l$ is the activation function and $W_l$ as a bias term.\n",
    "\n",
    "### (2.2) Kernel-Based Neural Operator\n",
    "\n",
    "The **kernel-based neural operator** extends this idea to function spaces. Instead of vector inputs, it learns mappings between discretized functions (e.g., on meshes or grids). Here, linear transformations are modeled as **integral operators**:\n",
    "$$\n",
    "v_{l+1}(x) = \\sigma_l\\left(\\int_\\Omega k_l(x, y) v_l(y) \\, dy + W_l v_l(x)\\right), \\tag{2.1}\n",
    "$$\n",
    "where:\n",
    "- $x, y \\in \\Omega$ are spatial points in the domain,\n",
    "- $k_l(x,y)$ is a learnable **kernel function**,\n",
    "- $\\sigma_l$ is the activation function,\n",
    "- and $W_l$ applies a pointwise linear transformation (bias).\n",
    "\n",
    "This kernel-based structure allows the model to process and learn **function-to-function** mappings directly, making it naturally suited for solving parametric PDE problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb94114-8c3a-451b-9c8f-7a17f948630c",
   "metadata": {},
   "source": [
    "## (3) What is the Fourier Neural Operator (FNO)?\n",
    "\n",
    "### (3.1) Motivation Behind FNO\n",
    "\n",
    "In computer vision, convolutional neural networks (CNNs) are highly effective because real-world images contain edges and local patterns, which CNNs can capture using **local convolution kernels**.\n",
    "\n",
    "However, in parametric PDEs, both the input coefficients and output solutions are continuous functions and often exhibit **global correlations** (e.g., smooth variations across the domain). Therefore, local convolutions may not be the most efficient approach.\n",
    "\n",
    "The **Fourier Neural Operator (FNO)** was developed to address this limitation by:\n",
    "- Replacing local convolution with **global convolution** using the Fourier transform;\n",
    "- Leveraging the fact that smooth functions (common in PDEs) are more compactly and efficiently represented in **Fourier space**.\n",
    "\n",
    "Thus, FNO chooses the kernel map $K$ to be a global convolution, implemented via **Fourier transformation**.\n",
    "\n",
    "### (3.2) The Fourier Layer: Core of FNO\n",
    "\n",
    "The Fourier layer is the fundamental building block of FNO. It enables efficient and expressive global transformations by working in the frequency domain.\n",
    "\n",
    "#### Why Fourier?\n",
    "- **Speed**: Direct integration over $n$ points is $\\mathcal{O}(n^2)$, but Fourier-based convolution is **quasilinear** in $n$ using the Fast Fourier Transform (FFT).\n",
    "- **Compactness**: PDE-related functions often have energy concentrated in lower frequency modes, so a small number of Fourier coefficients can encode a lot of information.\n",
    "\n",
    "#### How the Fourier Layer Works\n",
    "<center>\n",
    "    <img src=\"./Figures/FNO_fourier_layer.png\" width = \"800\" height='600' alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:gray\">\n",
    "      Fig 1: The Fourier layer\n",
    "  \t</div>\n",
    "</center>\n",
    "\n",
    "The Fourier layer replaces standard convolution with a three-step process:\n",
    "- **Step 1: Fourier Transform**: Convert the function $v_l$ from spatial domain to frequency domain using $\\mathcal{F}$.\n",
    "- **Step 2: Linear Transformation in Frequency Space**: Apply a learned transformation $R$ to the low-frequency modes.\n",
    "- **Step 3: Inverse Fourier Transform**: Map back to the spatial domain using $\\mathcal{F}^{-1}$.\n",
    "\n",
    "This gives:\n",
    "$$\n",
    "v_{l+1}(x) = \\sigma\\left( \\mathcal{F}^{-1}\\big(R \\cdot \\mathcal{F}(v_l)\\big)(x) + W v_l(x) \\right)\n",
    "$$\n",
    "where:\n",
    "- $\\sigma$ is a pointwise activation function (e.g., ReLU),\n",
    "- $R$ is a learnable weight matrix applied only on selected Fourier modes,\n",
    "- $W$ is a linear transformation applied in the spatial domain.\n",
    "\n",
    "#### Remarks:\n",
    "- **Remark A**: Truncation — In practice, we **discard high-frequency Fourier modes** and apply $R$ only on the lower modes, as they carry the most useful information for PDEs.\n",
    "- **Remark B**: Activation in Spatial Domain — Activations are applied after the inverse Fourier transform. This helps recover:\n",
    "    - **High-frequency details** omitted by truncation,\n",
    "    - **Non-periodic boundary behaviors** not captured by pure Fourier modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa745890-6ad8-408c-9d99-911765b3e430",
   "metadata": {},
   "source": [
    "### (3.3) Architecture of the Full FNO Model\n",
    "\n",
    "Now that we understand the Fourier layer, let’s look at how it fits into the full FNO framework for solving operator learning problems like:\n",
    "$$\n",
    "\\mathcal{G}: a(x, y) \\in \\mathcal{A} \\rightarrow u(x, y) \\in \\mathcal{U}\n",
    "\\tag{3.1}\n",
    "$$\n",
    "\n",
    "The complete architecture of the Fourier Neural Operator (FNO) is illustrated in Figure 1(a), with Figure 1(b) showing the internal mechanism of a single Fourier layer.\n",
    "<center>\n",
    "    <img src=\"./Figures/FNO_whole.jpeg\" width = \"800\" height='800' alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:gray\">\n",
    "      Fig 2: The FNO architecture\n",
    "  \t</div>\n",
    "</center>\n",
    "\n",
    "The FNO model consists of three main components:\n",
    "\n",
    "#### (3.3.1.) Input Layer: Lifting the Input Function\n",
    "To begin, the input coefficient function $a(x)$ (and optionally the coordinate $x$) is lifted into a higher-dimensional representation using a neural network $P_\\theta$. This creates the initial latent vector $v_0$, which serves as the input to the first Fourier layer:\n",
    "$$\n",
    "v_0 = P_\\theta(a, x)\n",
    "$$\n",
    "This step allows the model to map the input from function space into a latent space where the Fourier layers can operate more effectively.\n",
    "\n",
    "#### (3.3.2.) Fourier Layers: Learning Global Transformations\n",
    "The core of FNO consists of multiple stacked Fourier layers, which learn global representations efficiently in the frequency domain.\n",
    "\n",
    "Each layer updates the latent representation $v_l$ using the Fourier-layer operation:\n",
    "$$\n",
    "v_{l+1} = \\sigma\\left( \\mathcal{F}^{-1}(R \\cdot \\mathcal{F}(v_l)) + W v_l \\right)\n",
    "$$\n",
    "- $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote the Fourier and inverse Fourier transforms.\n",
    "- $R$ is a learnable matrix applied to selected low-frequency modes.\n",
    "- $W$ is a spatial-domain linear transformation.\n",
    "- $\\sigma$ is a nonlinear activation function (e.g., ReLU).\n",
    "\n",
    "By stacking several Fourier layers, the model can capture both low-frequency (smooth, global) and high-frequency (sharp, local) behavior of PDE solutions.\n",
    "\n",
    "#### (3.3.3.) Output Layer: Projecting Back to Function Space\n",
    "Finally, the output from the last Fourier layer $v_L$ is passed through another neural network $Q_\\theta$, which maps the latent representation back to the physical solution space and produces the predicted solution $u(x)$:\n",
    "$$\n",
    "u = Q_\\theta(v_L)\n",
    "$$\n",
    "This layer ensures that the model output has the same format and resolution as the ground truth solution of the PDE.\n",
    "\n",
    "#### Summary:\n",
    "The FNO architecture follows a simple but powerful design:\n",
    "<div style=\"background-color: #f0f8ff; padding: 1em; border-radius: 8px; text-align: center; width: 80%; margin: 1em auto; font-style: italic; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
    "    \n",
    "**Input function** $a(x)$ → **Latent encoding** via $P_\\theta$ → **Fourier layers for global transformation** → **Final decoding** via $Q_\\theta$ → **Predicted solution** $u(x)$\n",
    "\n",
    "</div>\n",
    "It combines the expressiveness of deep networks with the efficiency of Fourier methods, making it especially suitable for learning operators from parametric PDE problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be349970-d57b-409b-97ac-a27ed61adbd7",
   "metadata": {},
   "source": [
    "## (4) The FNO Procedure\n",
    "\n",
    "The Fourier Neural Operator (FNO) is a **data-driven**, **supervised learning** method designed to approximate the solution operator of parametric PDEs. The overall training and inference pipeline can be broken down into the following steps:\n",
    "### Step 1: Prepare Labeled Training Data\n",
    "\n",
    "FNO learns from a dataset of input-output function pairs $(a, u)$, discretized on **a fixed regular mesh grid**.\n",
    "- Let the **(regular) mesh** be denoted by $\\Xi = \\{\\xi_1, \\xi_2, \\dots, \\xi_n\\}$.\n",
    "- Each input function $a$ and its corresponding solution $u$ are evaluated on $\\Xi$, producing:\n",
    "$a(\\Xi), u(\\Xi)$.\n",
    "- The dataset consists of $N_{\\text{data}}$ such pairs:\n",
    "  $$\n",
    "    \\mathcal{D} = \\{(a^{(i)}(\\Xi), u^{(i)}(\\Xi))\\}_{i=1}^{N{\\text{data}}}\n",
    "  $$\n",
    "\n",
    "**Note**: Since FNO uses Fourier transforms, it requires a **uniform grid**. Inputs $a$ can be sampled from a function space $\\mathcal{A}$, while outputs $u$ are typically computed using accurate solvers (e.g., FEM or FDM).\n",
    "\n",
    "### Step 2: Build the FNO Model\n",
    "\n",
    "The FNO model $\\mathcal{G}_\\theta$ learns to approximate the true operator $\\mathcal{G}$. The forward pass through the model proceeds as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_0 &= P_\\theta(a(\\Xi)) \\quad \\text{(input lifting)} \\\\\n",
    "v_{l+1} &= \\sigma\\left(\\mathcal{F}^{-1}(R \\cdot \\mathcal{F}(v_l)) + Wv_l\\right), \\quad l = 0, \\dots, L-1 \\\\\n",
    "u(\\Xi) &= Q_\\theta(v_L) \\quad \\text{(projection to output)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here:\n",
    "- $P_\\theta, Q_\\theta$: input/output networks\n",
    "- $R, W$: learnable parameters in Fourier layers\n",
    "- $\\sigma$: activation function (e.g., ReLU)\n",
    "\n",
    "### Step 3: Define the Loss Function\n",
    "\n",
    "To train the model, we compare its predictions with the true solutions using the mean squared error (MSE):\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} \\, L(\\theta) = \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} \\left| \\mathcal{G}_\\theta(a^{(i)}(\\Xi)) - u^{(i)}(\\Xi) \\right|^2\n",
    "$$\n",
    "\n",
    "**Note**: Other loss functions (e.g., relative error, Sobolev norms) can also be used depending on the problem.\n",
    "\n",
    "### Step 4: Train the Model\n",
    "\n",
    "The parameters $\\theta$ are optimized using stochastic gradient descent (SGD) or its variants (e.g., Adam). The weights are updated iteratively as:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - l_r \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "- $l_r$: learning rate\n",
    "- $t$: training step \n",
    "\n",
    "Training continues until the loss converges or a stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd4492-b717-40d9-a005-c418e5fcab95",
   "metadata": {},
   "source": [
    "## (5) When Should We Use FNO? – Pros and Cons\n",
    "\n",
    "### (5.1) Advantages of FNO\n",
    "- **High accuracy and efficiency**: FNO typically outperforms DeepONet in both prediction accuracy and computational speed.\n",
    "- **Fast inference**: Once trained, FNO can rapidly solve PDEs for any new input $a \\in \\mathcal{A}$, making it suitable for real-time or many-query applications.\n",
    "\n",
    "### (5.2) Disadvantages of FNO\n",
    "- **Data inefficiency**: Like other deep operator networks, FNO requires a large number of labeled training pairs $(a, u)$, which may involve expensive numerical simulations or experiments.\n",
    "- **Limited generalization**: FNO may struggle to generalize well when the input $a$ is outside the distribution seen during training.\n",
    "- **Mesh-dependence**: FNO relies on a regular grid for applying the Fourier transform, making it less flexible for problems on irregular domains or with complex geometries. It may also have reduced accuracy at non-mesh locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
